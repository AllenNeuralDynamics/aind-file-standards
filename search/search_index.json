{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"AIND File Standards","text":"<p>This repository contains the file standards for the Allen Institute for Neural Dynamics (AIND).</p>"},{"location":"#goal","title":"Goal","text":"<p>The overarching goal of this effort is to give users, developers, and any system that produces or consumes information within AIND a clear set of specifications for structuring acquired data. This will make it easier to validate acquisition pipelines, ensure a more consistent user experience, and equip everyone with a common set of tools for working with that data.</p>"},{"location":"#contributing","title":"Contributing","text":"<p>Contributions are welcome! These will likely fall into one of the following categories:</p> <ul> <li>New Proposals: If you have a new file format that you would like to add, please create a new markdown file in the <code>file_formats</code> directory and follow the existing format.</li> <li>Improvements to Existing Formats: If you have suggestions for improving existing file formats, please submit a pull request with your changes.</li> </ul> <p>For more information on how to submit contributions, refer to docs/core/contributing.md.</p>"},{"location":"core/contributing/","title":"AIND - File Formats Standards","text":""},{"location":"core/contributing/#goals","title":"Goals","text":""},{"location":"core/contributing/#pillars","title":"Pillars","text":"<p>1) Rely on open-source tools and standards    1) Prioritize the use of open-source standard formats over proprietary ones.    2) In cases where a standard already exists, implement it or derive from it (e.g. datetime as ISO 8601).</p> <p>2) Well-separated and independent    1) A standard for a file format should be self-contained and introduce as few dependencies as possible. (e.g. a behavior-video data format should not depend on metadata from a fiber photometry data format). However, we acknowledge that information necessary for the interpretability of the data (but ideally not for its processing) should be included as metadata elsewhere (e.g. camera settings used to acquire the video).</p> <p>3) Versioning    1) We shall follow Semantic Versioning 2.0.0    2) Major version: breaking changes to standard (e.g. a file changed format from text to an image, or a column in a tabular file was deleted).    3) Minor version: backward-compatible changes to standard (e.g. a new column added to a tabular format).    4) Patch version: backward-compatible bug fixes (e.g. a typo in the documentation).</p>"},{"location":"core/contributing/#how-to-contribute","title":"How to contribute","text":"<p>Useful standards should be relatively stable and have longevity (see <code>How standards proliferate</code> ). Yet, we acknowledge that formats will inevitably need to evolve as new data modalities and tools are developed. To accommodate these two needs, we propose following a structured process for contributing and updating standards.</p>"},{"location":"core/contributing/#follow-the-standard-template-structure","title":"Follow the standard template structure","text":"<p>Contributions for existing or new standards should follow the template structure outlined in the <code>Template.md</code> file. This will ensure that all standards are consistent and legible. On top of the template structure, each proposed change should also be consistent with the <code>Core Standards</code> principles included in this repository.</p>"},{"location":"core/contributing/#identify-a-need-to-change-the-standard-issue","title":"Identify a need to change the standard (Issue)","text":"<p>All changes to current standards or addition of new specifications should start with an issue. This issue should describe the motivation, scope, and potential impact of the change. It should also include a brief description of alternatives that were considered.</p> <p>A potential template for the issue could be adapted from (from harp-tech):</p> <pre><code>## Summary (General description of the change, should also list the affected standard(s))\n## Motivation (Why is the change necessary)\n## Detailed Design (Proposed change, if applicable)\n## Drawbacks / Potential impact (what can go wrong? Will it affect an existing standard?)\n## Alternatives\n## Unresolved Questions\n## Stakeholders (Who should be involved in the discussion/potentially affected by the chance)\n</code></pre> <p>The issue will serve as a discussion point for the community to engage and provide feedback on the proposed change.</p>"},{"location":"core/contributing/#propose-a-change-to-the-standard-pull-request","title":"Propose a change to the standard (Pull Request)","text":"<p>Once the issue has been discussed and consensus has been reached, a moderator can invite the contributor to submit a pull request with the proposed change. A given issue may result in more than a single pull request, depending on the scope of the change, but must always reference the issue that motivated the proposed change. Pull requests should be reviewed by at least one maintainer and, if possible, by a domain expert (e.g. stakeholders that will use the standard).</p> <p>A potential template for the pull request could be adapted from (from harp-tech):</p> <pre><code>## Summary\n## Motivation\n## Detailed Design\n## Relevant Issues\n</code></pre> <p>The content of the pull request should reflect the issue that motivated the change.</p>"},{"location":"core/contributing/#release-cycle","title":"Release cycle","text":"<p>To ensure that standards are updated in a timely manner and release notes are easy to follow we propose the following structure for updating standards:</p> <p>1- A pull-request should be opened as described in the previous section. The PR should identify potential stakeholders and reviewers. 2- Once reviewers and stakeholders approve the PR, dissemination of the planned changes should be made to the community to allow further feedback / contestation of the proposed changes. 3- After a period of time, we suggest two weeks, if no major changes are needed, the PR is merged to the main branch and the new standard is approved.</p>"},{"location":"core/contributing/#is-it-a-new-standard-or-just-documentation","title":"Is it a new standard or just documentation?","text":"<p>This repository is meant to track standards NOT project-specific documentation. This may appear a subtle distinction but it is critical to create an self-consistent ecosystem that can be easily referenced and reused across projects. If you are looking to document a project-specific implementation, please consider documenting it in a separate repository or within the project's own documentation.</p> <p>The distinction between project-specific documentation and standards can be summarized as follows:</p> Aspect Documentation of File Specifications Standards for File Specifications Purpose Describe how things are currently done, not how they should be done. Define a set of guidelines, best practices, and formal specifications to be adopted widely. Audience A small, specific group (e.g., project contributors, maintainers, or immediate collaborators). A larger community of users, developers, and organizations beyond the original project. Scope Project-specific; tailored to current implementation details. Broadly applicable; independent of a single project\u2019s current setup. Nature Descriptive \u2014 explains the \u201cas is\u201d state. Prescriptive \u2014 defines the \u201cshould be\u201d state. Flexibility Can change frequently to reflect updates in the project\u2019s processes. Changes less often; revisions go through a formal review and adoption process. Adoption (Hopefully) Limited to people directly involved with the project. Encourages adoption by multiple groups, ideally across organizations. Value Proposition Helps contributors understand and follow the current system. Offers clear benefits that justify alignment (e.g., interoperability, efficiency, consistency). Dependencies May reference project-specific tools, code, or infrastructure. Self-contained, minimizing dependency on a single project\u2019s tools or infrastructure. Longevity Tied to the project\u2019s lifecycle. Aims for long-term relevance, even if the originating project changes or ends. Relationship Should ideally follow established standards where possible. In the long term, should be informed by shared community practices and evolving needs. <p>You can use the following flowchart to help determine whether your contribution should be a standard or just documentation:</p> <pre><code>flowchart TD\n    A[\"Is it about file formats?\"] -- No --&gt; B[\"Documentation\"]\n    A -- Yes --&gt; C[\"Will the format be reused&lt;br/&gt;in the future or by others?\"]\n    C -- No --&gt; B\n    C -- Yes --&gt; D[\"Does it define file structure,&lt;br/&gt;schema, or organization?\"]\n    D -- No --&gt; B\n    F[\"Could an existing standard&lt;br/&gt;be used instead?\"] -- Yes --&gt; G[\"Documentation \u2014&lt;br/&gt;possibly redundant\"]\n    F -- No --&gt; H[\"Does it align with or extend&lt;br/&gt;existing standards?\"]\n    H -- Yes --&gt; I[\"Refactor existing standard\"]\n    H -- No --&gt; J[\"New Standard\"]\n    D -- Yes --&gt; F\n</code></pre>"},{"location":"core/core-standards/","title":"AIND - File Formats Standards","text":""},{"location":"core/core-standards/#core-standards","title":"Core Standards","text":"<p>The goal of this document is NOT to be exhaustive and opinionated. Instead, it should include a minimal list of common patterns that can be easily referenced and re-used across all data formats standards guaranteeing a minimal level of consistency and quality.</p>"},{"location":"core/core-standards/#filename-conventions","title":"Filename conventions","text":"<p>In general, filename conventions will be defined by the specific data format standard. However, some general rules will be enforced:</p> <ul> <li>Filenames must not contain spaces or special characters. Use this as a reference for special characters.</li> <li>\"Underscore\" <code>_</code> should be used instead of \"-\" or any other special character to separate words.</li> <li>Filenames must always contain a file extension.</li> <li> <p>Any file name can be suffixed with a <code>datetime</code>. This suffix will ALWAYS be the last suffix in the filename, in case multiple suffixes are used, and will follow the ISO 8601 format. Following Scientific Computing guidelines, if a <code>datetime</code> field is added we will adopt the [format <code>YYYY-MM-DDTHHMMSS</code> e.g. <code>2023-12-25T133015</code>]. The <code>datetime</code> will always be read as local-time zone. Should universal time (i.e. UTC) or time-zone information be necessary, users should look into the metadata asset potentially generated with the data.</p> </li> <li> <p>As an example, if two files (<code>data_stream.bin</code>) are generated as part of two different acquisition streams:</p> <pre><code>   \ud83d\udcc2Modality\n    \u2523 \ud83d\udcdcdata_stream_2023-12-25T133015.bin\n    \u2517 \ud83d\udcdcdata_stream_2023-12-25T145235.bin\n</code></pre> </li> <li> <p>This rule can be generalized to container-like file formats by adding the suffix to the container:</p> <pre><code>    \ud83d\udcc2Modality\n    \u2523 \ud83d\udcc2FileContainer_2023-12-25T133015\n    \u2503 \u2523 \ud83d\udcdcfile1.bin\n    \u2503 \u2517 \ud83d\udcdcfile2.csv\n    \u2523 \ud83d\udcc2FileContainer_2023-12-25T145235\n    \u2503 \u2523 \ud83d\udcdcfile1.bin\n    \u2517 \u2517 \ud83d\udcdcfile2.csv\n</code></pre> </li> </ul>"},{"location":"core/core-standards/#datetime","title":"Datetime","text":"<p>All <code>datetime</code> used in data formats should follow the ISO 8601 standard. This standard is widely used and supported by most programming languages and libraries. In most cases, we expect <code>datetime</code> to be timezone aware, however if no suffix is added, <code>datetime</code> will be considered time-zone unaware and representing local time.</p> <p>The following formats are supported by the ISO 8601 standard and can be used:</p> <pre><code>YYYY-MM-DDTHHMMSS e.g. 2023-12-25T133015\n\nYYYY-MM-DDTHHMMSSZ e.g. 2023-12-25T133015Z\n\nYYYY-MM-DDTHHMMSS\u00b1HHMM e.g. 2023-12-25T133015+1200\n</code></pre> <p>The following examples show how to parse these formats in Python:</p> <pre><code>import datetime\n\ntz_unaware = \"2023-12-25T133015\"\nutc = \"2023-12-25T133015Z\"\ntz_aware = \"2023-12-25T133015+1200\"\n\nprint(datetime.datetime.fromisoformat(tz_unaware))\n#  2023-12-25 13:30:15\nprint(datetime.datetime.fromisoformat(utc))\n#  2023-12-25 13:30:15+00:00\nprint(datetime.datetime.fromisoformat(tz_aware))\n#  2023-12-25 13:30:15+12:00\n</code></pre>"},{"location":"core/core-standards/#tabular-formats","title":"Tabular formats","text":"<p>The supported tabular formats are:</p>"},{"location":"core/core-standards/#comma-separated-values-csv","title":"Comma-separated values (CSV)","text":"<p>CSV files will follow a subset of the RFC 4180 standard. The following rules will be enforced:</p> <ul> <li>The first row will always be the header row.</li> <li>The separator will always be a comma <code>,</code>.</li> <li>The file will always be encoded in UTF-8.</li> <li>The extension of the file will always be <code>.csv</code>.</li> </ul>"},{"location":"core/template/","title":"Standards on <code>&lt;data-format/modality&gt;</code> acquisition","text":""},{"location":"core/template/#version","title":"Version","text":"<p><code>&lt;SemVer&gt;(e.g. 0.1.0)</code> See Contributing.md for more information on versioning.</p>"},{"location":"core/template/#introduction","title":"Introduction","text":"<p>This section should briefly introduce the data format and its purpose.</p>"},{"location":"core/template/#raw-data-format","title":"Raw Data Format","text":""},{"location":"core/template/#file-format","title":"File format","text":"<p>This section describes the raw data format of the asset. Data is considered in its Raw format when it is directly acquired from the hardware and logged without any lossy / compression transformation. The resulting data asset will be considered immutable.</p> <p>The section should also include a brief description of the folder directory that results from the generation of the data asset. We recommend using the <code>file-tree-generator</code> vscode extension or the <code>tree</code> command in the terminal to generate a tree structure of the data asset.</p> <p>e.g.:</p> <pre><code>\ud83d\udce6behavior\n\u2523 \ud83d\udcc2foo\n\u2503  \u2517 \ud83d\udcc2bar_datetime\n\u2503     \u2523 \ud83d\udcdcbaz.dat\n\u2503     \u2517 \ud83d\udcdcbaz_metadata.txt\n...\n</code></pre>"},{"location":"core/template/#application-notes","title":"Application notes","text":"<p>This section is reserved to provide additional information on how to acquire the data in the data format described above. It can include information relative to the hardware (e.g. supported models), software interface (e.g. SoftwareFoo with version &gt;= 1.0.0), ideally, some easy to deploy or follow examples that can get anyone to reproduce the data format.</p>"},{"location":"core/template/#relationship-to-aind-data-schema","title":"Relationship to aind-data-schema","text":"<p>This section is reserved to describe how the data format relates and/or is represented by the aind-data-schema library. Examples include how the hardware and software metadata information should be encoded in the schemas, how data acquired should exist as <code>Epochs</code> and other critical conventions. It is important to note that this section should create a one-way dependency, where data formats should NOT depend on the schema and instead stand on their own. <code>aind-data-schema</code> should instead be used to provide extra information, context and validation to the data asset.</p>"},{"location":"core/template/#file-quality-assurances","title":"File Quality Assurances","text":"<p>This section is reserved to describe what features of the data format should be true if the data asset is to be considered valid. Conceptually, this section should describe features that can be easily tested and validated by unit tests. Examples include: - \"There will always be two files: <code>data.dat</code> and <code>metadata.txt</code>\" - \"For each frame in <code>video.avi</code>, there will be a corresponding row in <code>metadata.csv</code>\" - \"Field <code>Bar</code> will always be a positive integer\" - \"The first timestamp value in <code>metadata_camera.csv</code> will always be greater than the first one in <code>metadata_behavior.csv</code> - The <code>Time</code> column in <code>file.bin</code> is assumed to be aligned (sharing the same time domain) with <code>Time</code> column of <code>another_file.csv</code></p>"},{"location":"core/template/#primary-data-format","title":"Primary Data Format","text":""},{"location":"core/template/#file-format_1","title":"File format","text":"<p>This section describes the primary data format of the asset, which is the format of the data as it is uploaded. Primary data can have minimal processing applied, usually a compression or file format transformation. This section describes that transformation (if any), and the format of the resulting data. Similarly to the raw data format, it is considered immutable.</p>"},{"location":"core/template/#application-notes_1","title":"Application notes","text":"<p>Identical to the <code>Application notes</code> section in the <code>Acquisition/Raw Data Format</code> section. It should ideally contain information on how to generate the primary data format from the raw data format.</p>"},{"location":"core/template/#relationship-to-aind-data-schema_1","title":"Relationship to aind-data-schema","text":"<p>Identical to the <code>Relationship to aind-data-schema Session</code> section in the <code>Acquisition/Raw Data Format</code> section.</p>"},{"location":"core/template/#file-quality-assurances_1","title":"File Quality Assurances","text":"<p>Identical to the <code>File Quality Assurances</code> section in the <code>Acquisition/Raw Data Format</code> section.</p>"},{"location":"core/template/#derived-data-format","title":"Derived Data Format","text":""},{"location":"core/template/#file-format_2","title":"File format","text":"<p>This section is reserved to describe any derived data format from the primary data format. Derived data formats are considered to be post-processed data assets (potentially lossy) that are generated from the raw or primary data format. These are generally generated after the data has been acquired and uploaded. While immutable after created, derived data can be regenerated from primary data assets.</p>"},{"location":"core/template/#application-notes_2","title":"Application notes","text":"<p>Identical to the <code>Application notes</code> section in the <code>Acquisition/Raw Data Format</code> section. It should ideally contain information on how to generate the derived data format from the raw and/or primary data format.</p>"},{"location":"core/template/#relationship-to-aind-data-schema_2","title":"Relationship to aind-data-schema","text":"<p>Identical to the <code>Relationship to aind-data-schema Session</code> section in the <code>Acquisition/Raw Data Format</code> section.</p>"},{"location":"core/template/#file-quality-assurances_2","title":"File Quality Assurances","text":"<p>Identical to the <code>File Quality Assurances</code> section in the <code>Acquisition/Raw Data Format</code> section.</p>"},{"location":"file_formats/behavior_videos/","title":"Standards on behavior video acquisition","text":""},{"location":"file_formats/behavior_videos/#version","title":"Version","text":"<p>0.2.0</p>"},{"location":"file_formats/behavior_videos/#introduction","title":"Introduction","text":"<p>This document describes the standards for acquiring video data from behavior experiments. The goal is to ensure that the data is correctly acquired, logged, and stored in a way that is compatible with AIND's data processing pipelines. We will draw the line on including metadata that relates to the video data itself and NOT to the hardware or software that acquired it. This is to ensure that the data format is self-contained, maintainable and potentially reusable by other applications.</p>"},{"location":"file_formats/behavior_videos/#acquisitionrawprimary-data-format","title":"Acquisition/Raw/Primary Data Format","text":"<p>Following SciComp standards, video data from behavior experiments should be saved to the <code>behavior-videos</code> modality folder.</p> <p>Inside this folder, each camera should have its own directory, named <code>&lt;CameraName&gt;</code>. Inside each camera folder, there should be two files: <code>video.&lt;extension&gt;</code> and <code>metadata.csv</code>. The <code>video.&lt;extension&gt;</code> file should contain the video data, and the <code>metadata.csv</code> file should contain the metadata for the video.</p> <p><code>&lt;CameraName&gt;</code> is expected to match the name defined in the rig metadata file (<code>rig.json</code>)</p> <p>The folder structure will thus be:</p> <pre><code>\ud83d\udce6behavior-videos\n\u2523 \ud83d\udcc2BodyCamera\n\u2503 \u2523 \ud83d\udcdcmetadata.csv\n\u2503 \u2517 \ud83d\udcdcvideo.mp4\n\u2517 \ud83d\udcc2FaceCamera\n\u2503 \u2523 \ud83d\udcdcmetadata.csv\n\u2503 \u2517 \ud83d\udcdcvideo.mp4\n</code></pre> <p>If multiple streams from the same camera are acquired in the same session, an optional <code>datetime</code> suffix can be added to the container's name:</p> <pre><code>\ud83d\udce6behavior-videos\n\u2523 \ud83d\udcc2BodyCamera_2023-12-25T133015\n\u2503 \u2523 \ud83d\udcdcmetadata.cs\n\u2503 \u2517 \ud83d\udcdcvideo.mp4\n\u2517 \ud83d\udcc2BodyCamera_2023-12-25T145001\n\u2503 \u2523 \ud83d\udcdcmetadata.csv\n\u2503 \u2517 \ud83d\udcdcvideo.mp4\n</code></pre> <p>The metadata file is expected to contain the following columns:</p> <ul> <li> <p><code>ReferenceTime</code> - Time of the trigger given by hardware (e.g. Harp)</p> </li> <li> <p><code>CameraFrameNumber</code> \u2013 Frame counter given by the camera API or manually added by user (e.g. using OS counter for webcams)</p> </li> <li> <p><code>CameraFrameTime</code> \u2013 Frame acquisition time given by the camera API or manually added by the user (e.g. using OS scheduler for webcams).</p> </li> </ul> <p>As for the video, since the format will depend on the scientific question and software/hardware constrains, we will not enforce a specification. However, we strongly discourage the use of RAW, uncompressed data, and should the user not have a preference, we suggest the follow default:</p> <p>Use a separate online encoder and offline encoder for use during acquisition, and long-term storage, respectively</p> <p>For the online encoder:</p> <ul> <li>Acquire without any gamma correction</li> <li>Acquire with the mkv format so that files are not corrupted if acquisition is   abnormally terminated, i.e. the video files should be named like <code>video.mkv</code></li> <li>Use <code>ffmpeg</code> with the following encoding codec string for online encoding (optimized for compression quality and speed):</li> </ul> <p>Note: this has been tested with monochrome videos with the raw pixel format   <code>gray</code>. For color videos, the input arguments might need to be altered to   match the color space of the input.</p> <ul> <li>output arguments: <code>-vf \"scale=out_range=full,setparams=range=full:colorspace=bt709:color_primaries=bt709:color_trc=linear\" -c:v h264_nvenc -pix_fmt yuv420p -color_range full -colorspace bt709 -color_trc linear -tune hq -preset p3 -rc vbr -cq 18 -b:v 0M -metadata author=\"Allen Institute for Neural Dynamics\" -maxrate 700M -bufsize 350M -f matroska -write_crc32 0</code></li> <li>input_arguments: <code>-colorspace bt709 -color_primaries bt709 -color_range full -color_trc linear</code></li> </ul> <p>For offline re-encoding (optimized for quality and size):</p> <ul> <li>Use mp4 container for the final video, i.e. the video should be named like <code>video.mp4</code>.</li> <li>output arguments: <code>-vf \"scale=out_color_matrix=bt709:out_range=full:sws_dither=none,format=yuv420p10le,colorspace=ispace=bt709:all=bt709:dither=none,scale=out_range=tv:sws_dither=none,format=yuv420p\" -c:v libx264 -preset veryslow -crf 18 -pix_fmt yuv420p -metadata author=\"Allen Institute for Neural Dynamics\" -movflags +faststart+write_colr</code></li> </ul>"},{"location":"file_formats/behavior_videos/#higher-bit-depth-recordings","title":"Higher bit-depth recordings","text":"<p>Note: this hasn't been tested as thoroughly.</p> <p>For higher bit depth (more than eight) recordings, change the output arguments of the online encoding to be as follows:   - output arguments: <code>-vf \"format=yuv420p10le,scale=out_range=full,setparams=range=full:colorspace=bt709:color_primaries=bt709:color_trc=linear\" -c:v hevc_nvenc -pix_fmt p010le -color_range full -colorspace bt709 -color_trc linear -tune hq -preset p4 -rc vbr -cq 12 -b:v 0M -metadata author=\"Allen Institute for Neural Dynamics\" -maxrate 700M -bufsize 350M -f matroska -write_crc32 0</code></p> <p>The HEVC encoder must be used to support 10 bit depth, and the pixel format has been changed to <code>p010le</code> which is a yuv420-like 10 bit pixel format that is accepted by NVENC. We also recommend using <code>.mkv</code> videos at the rig, to reduce the risk of data loss.</p> <p>Note that this saves the pixel data at 10 bit depth, even if the camera is acquiring 12 or higher. NVENC does not support saving more than 10 bit pixel depths. However, saving 10 bit pixel depth before gamma encoding will result in more accurate gamma encoding for the second stage encoding.</p> <p>There is an intermediate pixel format, yuv420p10le, which is necessary at the time of writing for gray pixel format inputs due to incorrect chroma initialization for p010le. Depending on your pixel format, and recent changes to ffmpeg, this may not be necessary.</p> <p>For the video to retain 10 bit depth for long-term storage, the offline encoder will also need to be changed. For example, set the output arguments to: <pre><code>-vf \"colorspace=ispace=bt709:all=bt709:dither=none,scale=out_range=tv:sws_dither=none,format=yuv420p10le\"\n-c:v libx264 -preset veryslow -crf 18 -pix_fmt yuv420p10le\n-metadata author=\"Allen Institute for Neural Dynamics\" -movflags +faststart+write_colr\n</code></pre></p> <p>However, acquiring at 10 bits, gamma encoding, and saving 8 bit-depth videos for long-term storage is sufficient for many applications.</p>"},{"location":"file_formats/behavior_videos/#application-notes","title":"Application notes","text":"<p>We currently support the following cameras:     - <code>Blackfly S BFS-U3-16S2M</code>     - <code>Blackfly S BFS-U3-04S2M</code></p> <p>Additional cameras could be supported but the user should provide the necessary information to integrate it with the current pipeline.</p> <p>Caution</p> <p>It is the user's responsibility to ensure that:</p> <ul> <li> <p>The camera is correctly calibrated and that the settings are appropriate for the experiment.</p> </li> <li> <p>Unless there is a reason not to, the default logging pattern should always follow the following logic: (Stop trigger if needed) -&gt; Start logging -&gt; Start Camera -&gt; Start Trigger -&gt; Acquire data -&gt; Stop Trigger -&gt; Stop Logging. This guarantees that all relevant events are recorded.</p> </li> <li> <p>Trigger generation only starts AFTER the camera hardware has been initialized. This is to ensure that the camera is ready to receive the first trigger signal.</p> </li> <li> <p>For each trigger of the trigger source (e.g. Harp Behavior board) a corresponding camera exposure should occur. One example where this can be violated is if the set exposure is greater than the trigger frequency.</p> </li> <li> <p>In absence of dropped frames (defined as skips in the FrameNumber &gt; 1) the metadata.csv file is expected to be aligned with the video file.</p> </li> <li> <p>(Optional) Start trigger and Stop trigger events should be available for QC.</p> </li> <li> <p>(Optional) The logs of all triggers (regardless of whether they are logged in the metadata.csv) should be saved for redundancy.</p> </li> </ul>"},{"location":"file_formats/behavior_videos/#acquisition-and-logging","title":"Acquisition and Logging","text":"<p>Acquisition can be made using Bonsai. An operator that instantiates the camera can be found in AllenNeuralDynamics.Core package. This operator is a wrapper around the Spinnaker SDK and provides a simple interface to acquire video data. Since it forces the camera into the correct settings (e.g. Trigger mode, disabled gamma correction, etc...), it guarantees that camera metadata is static and thus easier to track.</p> <p>Logging can be implemented via the FFMPEG operator.</p> <p>While we suggest using the aforementioned recipes, the user is free to use any software that can acquire video data, provided it is validated and logged in the correct format.</p>"},{"location":"file_formats/behavior_videos/#relationship-to-aind-data-schema","title":"Relationship to aind-data-schema","text":"<p><code>&lt;CameraName&gt;</code> is expected to match the name defined in the rig metadata file (<code>rig.json</code>). Several fields in the metadata can be automatically extracted from this file format (e.g. start and stop of the stream, resolution of the video). However, the user should ensure that any data pertaining to the hardware configuration (e.g. camera model, exposure time, gain, camera position, etc...) is logged indepedently from this file format herein described.</p>"},{"location":"file_formats/behavior_videos/#file-quality-assurances","title":"File Quality Assurances","text":"<p>The following features should be true if the data asset is to be considered valid:</p> <ul> <li> <p>The number of frames in the encoded video should match the number of recorded frames and the number of frames in the metadata.</p> </li> <li> <p>Check if dropped frames occurred. This should be done in two ways:</p> </li> <li> <p>The difference between adjacent FrameNumber is always 1;</p> </li> <li> <p>The difference between adjacent Seconds and adjacent FrameTime should be very close (I would suggest a threshold of 0.5ms for now);</p> </li> </ul> <p>Note</p> <p>While dropped frames are not ideal, they do not necessarily invalidate the data. However, the user should be aware of the potential consequences and/or ways to correct the data asset.</p> <ul> <li>If using a stable frame rate (this should be inferred from a rig configuration file), the average frame rate should match the theoretical frame rate;</li> </ul> <p>(optional) If the optional start and stop events are provided, the following temporal order should be asserted: <code>All(StartTrigger &lt; Frames  &lt; StopTrigger&gt;)</code></p>"},{"location":"file_formats/ecephys/","title":"Standards on electrophysiology acquisition","text":"<p>Version: 0.0.2</p>"},{"location":"file_formats/ecephys/#introduction","title":"Introduction","text":"<p>This document describes the standards for the acquisition of electrophysiology data in the Allen Institute for Neural Dynamics, which primarily consists of extracellular electrophysiology signals acquired using Neuropixels probes.</p>"},{"location":"file_formats/ecephys/#raw-data-format","title":"Raw Data Format","text":"<p>Following SciComp standards, ecephys data from experiments should be saved to the \"ecephys\" modality folder.</p>"},{"location":"file_formats/ecephys/#file-format","title":"File format","text":"<p>The raw data format is a folder produced by the Open Ephys GUI (version&gt;=0.6.0) in binary format and is organized as outlined in the official Open Ephys Documentation.</p> <p>In addition, it is required that the Open Ephys folder is temporally aligned using the <code>generate_report</code> or the <code>align_timestamps</code> functions from the aind-ephys-rig-qc package to ensure that different streams are synced  with each other either locally or to the HARP clock (see HARP format). Time alignment will produce additional timestamps files for each continuous/event stream: - <code>original_timestamps.npy</code> - <code>localsync_timestamps.npy</code> (only if HARP clock is detected)</p>"},{"location":"file_formats/ecephys/#application-notes","title":"Application notes","text":"<p>The raw data can be directly read using the open-ephys-python-tools or the SpikeInterface package.</p>"},{"location":"file_formats/ecephys/#relationship-to-aind-data-schema","title":"Relationship to aind-data-schema","text":"<p>The <code>rig.json</code> will contain relevant metadata from the Open Ephys settings, such as the probe names and serial numbers.</p>"},{"location":"file_formats/ecephys/#file-quality-assurance-and-assumptions","title":"File Quality Assurance and Assumptions","text":"<p>The electrophysiology data is visually inspected during the experiments. In addition, after a session is acquired, a quality control report is generated using the aind-ephys-rig-qc <code>generate_qc_report</code> function. This report is used to assess the quality of the data and to identify potential issues with the acquisition, including timestamps misalignments, abnormal noise levels and power spectra, and excessive drift.</p>"},{"location":"file_formats/ecephys/#primary-data-format","title":"Primary Data Format","text":""},{"location":"file_formats/ecephys/#file-format_1","title":"File format","text":"<p>Before the data is uploaded to the cloud, it undergoes a data transformation to compress  the raw data using the aind-data-transformation.  The original Open Ephys folder is kept, but the <code>.dat</code> files  are clipped to reduce the file size maintaining maintain the original folder structure. This clipped Open  Ephys folder is saved in the <code>ecephys_clipped</code> folder. The <code>.dat</code> files are compressed to <code>zarr</code> using the  SpikeInterface library. The compressed data is saved in the <code>ecephys_compressed</code> folder.</p> <p>The primary data format is organized as follows:</p> <pre><code>\ud83d\udce6ecephys\n\u2523 \ud83d\udcc2ecephys_clipped\n\u2503 \u2517 \ud83d\udcc2&lt;Record Node *I*&gt;\n\u2503   \u2517 \ud83d\udcc2&lt;experiment*J*&gt;\n\u2503     \u2523 \ud83d\udcc2&lt;recording*K*&gt;\n\u2503     \u2503 \u2523 \ud83d\udcc2&lt;continuous&gt;\n\u2503     \u2503 \u2503 \u2517 \ud83d\udcc2&lt;StreamName&gt;\n\u2503     \u2503 \u2503   \u2523 \ud83d\udcdcclipped continuous.dat\n\u2503     \u2503 \u2503   \u2517 \ud83d\udcdcother intact files \n\u2503     \u2503 \u2523 \ud83d\udcc2&lt;events&gt;\n\u2503     \u2503 \u2503 \u2517 \ud83d\udcc2&lt;StreamName&gt;\n\u2503     \u2503 \u2503   \u2517 \ud83d\udcdc original event files\n\u2503     \u2503 \u2517 \ud83d\udcdc structure.oebin\n\u2503     \u2517 \ud83d\udcdc settings.xml\n\u2517 \ud83d\udcc2ecephys_compressed\n  \u2523 \ud83d\udcc2&lt;experiment*J*_Record Node*I*#StreamName.zarr&gt;\n  \u2503 \u2517 \ud83d\udcc2channel_ids\n  \u2503 \u2517 \ud83d\udcc2properties\n  \u2503 \u2517 \ud83d\udcc2times_seg*K-1*\n  \u2503 \u2517 \ud83d\udcc2traces_seg*K-1*\n  \u2503 \u2517 \ud83d\udcdc.zattrs\n  \u2517 ...\n</code></pre> <p>The <code>ecephys_clipped</code> folder contains all the original Open Ephys folders and files, with the only difference that  the <code>.dat</code> files are clipped to 100 samples. Therefore, this folder can be opened by the normal sowtware tools  that can read Open Ephys data, such as the open-ephys-python-tools and the SpikeInterface library.</p> <p>The <code>ecephys_compressed</code> folder contains the compressed data in the <code>zarr</code> format.  Data from different Open Ephys \"experiments\" and different streams are saved in different <code>.zarr</code> files. The <code>zarr</code> file is produced from the <code>spikeinterface.BaseRecording.save_to_zarr()</code> function,  which saves and compresses the data and its metadata to a <code>.zarr</code> file. In case of multiple Open Ephys \"recordings\", the data is saved in different \"segments\" (e.g. <code>traces_seg0</code>, <code>traces_seg1</code>, etc.).</p> <p>Here is a brief description of the content of the <code>.zarr</code> file:</p> <ul> <li><code>channel_ids</code>: list of channel ids from the original Open Ephys data</li> <li><code>properties</code>: dictionary with the properties of the recording (e.g. <code>channel_locations</code>, <code>gains</code>, etc.)</li> <li><code>times_seg*K-1*</code>: Open Ephys timestamps for each segment</li> <li><code>traces_seg*K-1*</code>: compressed traces for each segment</li> </ul>"},{"location":"file_formats/ecephys/#compression-details","title":"Compression details","text":"<p>The <code>traces_seg*K-1*</code> is chunked in (30000, 384), so that each chunk corresponds to 1 second of data for all channels. By default, we use the <code>wavpack</code> codec, implemented as a <code>zarr</code> compressor  in the wavpack-numcodecs package. Prior to compression, the LSB (least significant bit) offset is subtracted from each channel, using the  <code>spikeinterface.preprocessing.correct_lsb()</code> function. This is done to ensure that the data is centered around 0, which improves the compression ratio. For more information, see Buccino et al. 2023.</p>"},{"location":"file_formats/ecephys/#application-notes_1","title":"Application notes","text":"<p>Here is a snippet of code that shows how to read the clipped and compressed data using the SpikeInterface library:</p> <pre><code>import spikeinterface as si\nimport spikeinterface.extractors as se\n\n# read number of blocks (experiments) and streams (probes) from the clipped data\nnum_blocks = se.get_neo_num_blocks(\"openephysbinary\", \"ecephys/ecephys_clipped\")\nstream_names, stream_ids = se.get_neo_streams(\"openephysbinary\", \"ecephys/ecephys_clipped\")\n\n# read the first block and stream\nblock_index = 0\nstream_name = stream_names[0]\nrecording_clipped = se.read_openephys(\n  \"ecephys/ecephys_clipped/\",\n  block_index=block_index,\n  stream_name=stream_name\n)\n\n# read the openephys events\nevents = se.read_openephys_events(\"ecephys/ecephys_clipped/\")\n\n# read the compressed data for the first block and stream\nrecording_compressed = si.read_zarr(\n  f\"ecephys/ecephys_compressed/experiment{block_index+1}_{stream_name}.zarr\"\n)\n</code></pre>"},{"location":"file_formats/ecephys/#relationship-to-aind-data-schema_1","title":"Relationship to aind-data-schema","text":"<p>The <code>processing.json</code> includes the compression details and parameters.</p>"},{"location":"file_formats/ecephys/#file-quality-assurance-and-assumptions_1","title":"File Quality Assurance and Assumptions","text":"<p>No further quality assurance is performed during the conversion from the raw data to the primary data format.</p>"},{"location":"file_formats/fip/","title":"Standards on FIP fiber photometry acquisition","text":""},{"location":"file_formats/fip/#version","title":"Version","text":"<p><code>0.4.0</code></p>"},{"location":"file_formats/fip/#introduction","title":"Introduction","text":"<p>This document describes the standards for the acquisition of frame-projected independent-fiber photometry (FIP) data in the Allen Institute for Neural Dynamics.</p>"},{"location":"file_formats/fip/#raw-data-format","title":"Raw Data Format","text":"<p>Following SciComp standards, FIP data should be saved in their own folder named \"fib\" (short for \"fiber photometry\"). Data from other modalities go in separate folders.</p>"},{"location":"file_formats/fip/#file-format","title":"File format","text":"<p>In most cases, FIP data will be saved in <code>CSV</code> files, where each file corresponds to a different channel in the photometry rig. In addition to the timeseries fluorescence data, files containing metadata and raw image data are also available. A single session of FIP data should be organized under a the <code>fib</code> directory. An acquisition for a single session should be nested under a sub directory named following the core standards for file naming convention found here.  Mostly, this is for cases where the recording gets interrupted. When the system restarts under the same session, it can be added to a new folder. A sessions folder structure should look like the following:</p> <pre><code>\ud83d\udce6 fib\n\u2523 \ud83d\udcc2 &lt;fip_YYYY-MM-DDTHHMMSS&gt;\n\u2503 \u2523 green.csv\n\u2503 \u2523 red.csv\n\u2503 \u2523 iso.csv\n\u2503 \u2523 green.bin\n\u2503 \u2523 red.bin\n\u2503 \u2523 iso.bin\n\u2503 \u2523 [Optional]background_green.bin\n\u2503 \u2523 [Optional]background_red.bin\n\u2503 \u2523 [Optional]background_iso.bin\n\u2503 \u2523 [Optional]background_green.csv\n\u2503 \u2523 [Optional]background_red.csv\n\u2503 \u2523 [Optional]background_iso.csv\n\u2503 \u2523 green_metadata.json\n\u2503 \u2523 red_metadata.json\n\u2503 \u2523 iso_metadata.json\n\u2503 \u2523 camera_green_iso_metadata.csv\n\u2503 \u2523 camera_red_metadata.csv\n\u2503 \u2517 regions.json\n\u2517 \ud83d\udcc2 &lt;fip_YYYY-MM-DDTHHMMSS&gt;\n  \u2523 green.csv\n  \u2523 &lt;...&gt;\n  \u2517 regions.json\n</code></pre> <p>Data is generally organized by the emission channel that gave rise to the data (<code>green</code>, <code>red</code>, and <code>iso</code>), respectively. For details on the rig setup, please refer to the data acquisition repository.</p>"},{"location":"file_formats/fip/#fluorescence-data","title":"Fluorescence data","text":"<p>Each fiber photometry session will primarily be analyzed by using the average signal from regions of interest (ROIs) placed on top of raw video frames during acquisition. To simplify analysis, we average the signal from all pixels within the ROIs during online acquisition and make it available as time series data in <code>CSV</code> files. These are <code>green.csv</code>, <code>red.csv</code>, and <code>iso.csv</code> files, respectively. All files share the same format, where each row corresponds to a single frame of the video and each column can be described as follows:</p> <ul> <li><code>ReferenceTime</code> Time of the trigger given by hardware (Harp)</li> <li><code>CameraFrameNumber</code> Frame counter given by the camera API</li> <li><code>CameraFrameTime</code> Frame acquisition time given by the camera API</li> <li><code>Background</code> CMOS dark count floor signal</li> <li><code>Fiber_0</code> Average signal values for Fiber_0's selected ROI</li> <li><code>&lt;...&gt;</code> (Variable number of columns)</li> <li><code>Fiber_N</code> Average signal values for Fiber_N's selected ROI.</li> </ul>"},{"location":"file_formats/fip/#raw-sensor-data","title":"Raw sensor data","text":"<p>Raw sensor data (i.e., camera frames) that generated the fluorescence data is saved in raw binary files. These files share the same naming convention as the fluorescence data files, but with a <code>.bin</code> extension. During acquisition, operators place circular ROIs over the images, and photometry readouts are obtained by averaging the signal inside these regions.</p> <p>To open these files, users need additional information to parse the binary data. Data is stored in a <code>ColumnMajor</code> layout format, where each frame can be parsed with the information available in the corresponding <code>.json</code> file. Each <code>.json</code> file contains the following fields:</p> <ul> <li><code>Width</code>: Imaging width (200 px by default)</li> <li><code>Height</code>: Imaging height (200 px by default)</li> <li><code>Depth</code>: Bit depth (U16 by default)</li> <li><code>Channel</code>: Channel (1 channel by default)</li> </ul> <p>See the Application Notes section for an example of how to parse the binary files.</p>"},{"location":"file_formats/fip/#sensor-background-data-optional","title":"Sensor background data (optional)","text":"<p>In some experiments, operators may choose to record background frames without any illumination. These frames can be used to estimate the camera's dark count floor and subtract it from the raw data during post-processing. If background frames are recorded, they will mirror the naming convention of the raw sensor data files and append <code>background_</code> to the beginning of the filename. While optional, these files are expected to still honor the <code>&lt;color&gt;_metadata.json</code> and <code>regions.json</code> specifications.</p> <p>If the background files are not present, users can assume that no background frames were recorded during the session.</p> <p>If the background files are present they must contain at least one frame (i.e. row) in the corresponding <code>&lt;color&gt;_background.csv</code> file.</p> <p>If one background file is present, all background files must be present.</p>"},{"location":"file_formats/fip/#recovering-the-regions-of-interest","title":"Recovering the regions of interest","text":"<p>The regions of interest (ROIs) used during the experiment are saved as a single <code>JSON</code> file named <code>regions.json</code>. Each ROI is defined as a <code>Circle</code> with a center coordinate (<code>[x,y]</code>) and a radius (<code>r</code>) in pixels. The units (pixels) are the same as in the parsed <code>.bin</code> file. This file contains the following fields:</p> <pre><code>regions.json\n\u251c\u2500\u2500 camera_green_iso_background: Circle[[x, y], r]\n\u251c\u2500\u2500 camera_red_background:       Circle[[x, y], r]\n\u251c\u2500\u2500 camera_green_iso_roi:        list of Circle[[x, y], r]\n\u251c\u2500\u2500 camera_red_roi:              list of Circle[[x, y], r]\n</code></pre> <p>An image like the one below can be generated by combining the previous two files.</p> <p></p>"},{"location":"file_formats/fip/#camera-metadata","title":"Camera Metadata","text":"<p>The fiber imaging system uses a single camera to capture data from two distinct light sources (<code>iso</code> and <code>green</code> channels) through temporal multiplexing. As a result, only two metadata files are generated - one for each camera. These files can be used to ensure data integrity during post-processing. The metadata files are named as follows:</p> <ul> <li><code>camera_green_iso_metadata.csv</code>: metadata from the camera recording both green and iso channels</li> <li><code>camera_red_metadata.csv</code>: metadata from the camera recording the red channel</li> </ul> <p>Within the metadata files are the following columns:</p> <ul> <li><code>ReferenceTime</code> Time of the trigger given by hardware (Harp)</li> <li><code>CameraFrameNumber</code> Frame counter given by the camera API</li> <li><code>CameraFrameTime</code> Frame acquisition time given by the camera API</li> <li><code>CpuTime</code> Software timestamp from the OS, in timezone-aware ISO8061 format. Users should consider these timestamps low-precision and rig-dependent, and should not rely on them for analysis.</li> </ul> <p>Under expected operating conditions, these files will contain all the rows present in the <code>&lt;color&gt;.csv</code> and <code>background_&lt;color&gt;.csv</code> files.</p>"},{"location":"file_formats/fip/#application-notes","title":"Application notes","text":""},{"location":"file_formats/fip/#parsing-raw-binary-files","title":"Parsing raw binary files","text":"<p>These files can be read by specifying the structure of the videos as follows:</p> <pre><code>import numpy as np\nimport json\n\n\ndef load_average_frame(video_file, start_frame, end_frame, frame_size, dtype, frame_width, frame_height):\n    \"\"\"\n    Parameters:\n        video_file (str): Path to the video file.\n        start_frame (int): Index of the starting frame.\n        end_frame (int): Index of the ending frame (exclusive).\n        frame_size (int): Byte size of a single frame.\n        dtype (numpy.dtype): Data type of the frame.\n        frame_width (int): Width of the frame.\n        frame_height (int): Height of the frame.\n\n    Returns:\n        numpy.ndarray: Average image (frame_height x frame_width).\n    \"\"\"\n    num_frames = end_frame - start_frame\n    if num_frames &lt;= 0:\n        raise ValueError(\"Invalid frame range specified.\")\n\n    accumulated_frame = np.zeros((frame_height, frame_width), dtype=np.float64)\n\n    with open(video_file, \"rb\") as f:\n        for frame_index in range(start_frame, end_frame):\n            f.seek(frame_index * frame_size)\n            frame_data = np.frombuffer(f.read(frame_size), dtype=dtype)\n\n            if frame_data.size != frame_width * frame_height:\n                raise ValueError(\"Reached end of file.\")\n\n            accumulated_frame += frame_data.reshape((frame_height, frame_width))\n\n    return (accumulated_frame / num_frames).astype(dtype)\n\n\nif __name__ == '__main__':\n    with open('red.meta', 'r') as j:\n        metadata = j.read()[0]\n    metadata = json.load(metadata)\n    video_fp = '/path/to/file.bin'\n    start_frame = 0\n    end_frame = 1000\n    average_frame = load_average_frame(\n        video_fp,\n        start_frame,\n        end_frame,\n        frame_width * frame_height * np.dtype(metadata['Depth']).itemsize,\n        np.dtype(metadata['Depth']),\n        metadata['Width'],\n        metadata['Height']\n    )\n</code></pre>"},{"location":"file_formats/fip/#acquiring-data-under-this-format","title":"Acquiring data under this format","text":"<p>Data acquisition code that generates data in this format is available from the data acquisition repository.</p>"},{"location":"file_formats/fip/#relationship-to-aind-data-schema","title":"Relationship to aind-data-schema","text":"<p>procedures.json documents the relevant fiber probe implantation metadata (stereotaxic coordinates) and viral injection metadata (stereotaxic coordinates, materials). session.json documents the intended measurement (e.g. norepinephrine, dopamine, etc) for each channel of each probe. </p>"},{"location":"file_formats/fip/#file-quality-assurances","title":"File Quality Assurances","text":"<p>The following are expected to be true for all FIP data collected under this standard:</p> <ul> <li>The number of frames in the raw binary files shall match the number of frames in the corresponding <code>CSV</code> files (e.g., <code>green.csv</code> and <code>green.bin</code>).</li> <li>The number of frames across all <code>CSV</code> files shall be the same (i.e., <code>green.csv</code> = <code>red.csv</code> = <code>iso.csv</code>) and, by extension, the number of frames in the corresponding binary files.</li> <li>Camera metadata files shall contain no dropped frames. This can be verified by checking the <code>CameraFrameNumber</code> column in the metadata files. The difference between consecutive frames must ALWAYS be 1. If a dropped frame is present, data may be corrupted and should be flagged for manual review.      &gt; [!WARNING]     &gt; Dropped frames are not normal and should not be taken lightly. If you encounter dropped frames, please contact the data acquisition team for further investigation.</li> <li>The difference between the derivative of <code>CameraFrameTime</code> and <code>ReferenceTime</code> is expected to be very small (i.e.: abs(max(diff(<code>CameraFrameTime</code>) - diff(<code>ReferenceTime</code>))) &lt; 0.2ms). If this is not the case, it may indicate a problem with frame exposure.</li> <li>All rows in the <code>&lt;color&gt;.csv</code> files will be present in the corresponding camera metadata files. The opposite is not guaranteed to be true.</li> <li>A <code>&lt;color&gt;.csv</code> file is not guaranteed to have a <code>Fiber_N</code> column. A <code>Background</code> column is always present. The order of the columns in the <code>&lt;color&gt;.csv</code> files is not guaranteed to be the same across different sessions. It is thus recommended to use the header as the index for the columns.</li> <li>The naming of <code>Fiber_&lt;i&gt;</code> columns in the <code>&lt;color&gt;.csv</code> files is guaranteed to be sequential, starting from <code>Fiber_0</code> and going up to <code>Fiber_N</code>.</li> <li>The <code>regions.json</code> in the FIP session are guaranteed to be static within a session. The number and order of the ROIs are expected to be the same across the two cameras.</li> </ul>"},{"location":"file_formats/harp/","title":"Standards on harp device data acquisition","text":""},{"location":"file_formats/harp/#version","title":"Version","text":"<p>0.2.0</p>"},{"location":"file_formats/harp/#introduction","title":"Introduction","text":"<p>While Harp data is largely used for behavior experiments, it is not limited to this modality. As a result, the current standard is scoped to the logging at the level of single Harp devices. The reason for this decision will hopefully become clear as we describe the format and some of the rationale behind it.</p> <p>Most of the Harp-related concepts mentioned here will be expanded in the documentation of the protocol.</p> <p>We will strictly follow the logging standards defined by Harp. This decision is justified by the following reasons:</p> <ul> <li>Affords the ability to not only use the same data format within our organization but also to share data with other groups that use Harp;</li> <li>Affords the ability to reuse common data ingesting tools maintained by others. Since it is an open-source community standard, we also have the ability to contribute to the development of these tools if we so wish;</li> <li>Affords re-usability of data acquisition, QC and processing pipelines that can be centralized and validated by us and potentially used by others.</li> </ul>"},{"location":"file_formats/harp/#acquisitionrawprimary-data-format","title":"Acquisition/Raw/Primary Data Format","text":"<p>One of the main advantages of using a standardized binary communication protocol is that logging data from harp devices can be largely generalized. In theory, we could simply dump the binary data from the device into a single file and call it a day. However this is not always the most convenient way to log data. For instance, if one is interested in ingesting only a subset of messages (e.g. only the messages from a particular sensor or pin), then the previous approach would require a post-processing step to filter out the messages of interest. Furthermore, each address, as per harp protocol spec, has potentially different data formats (e.g. <code>U8</code> vs <code>U16</code>) or even different lengths if array registers are involved. This can make it very tedious to parse and analyze a binary file offline, since we will have to examine the header of each and every message in the file to determine how to extract its contents.</p> <p>This analysis could be entirely eliminated if we knew that all messages in the binary file had the same format. For any Harp device, the payload stored in a specific register will have a fixed type and length. This means that to ensure our simplifying assumption it is enough to save each message from a specific register into a different file (aka de-multiplexing strategy).</p> <p>Thus, for each device, the container of all data will be a single directory with the extension <code>&lt;&gt;.harp</code>. This directory will contain the following files:</p> <pre><code>\ud83d\udce6&lt;Device&gt;.harp\n \u2523 \ud83d\udcdc&lt;DeviceName&gt;_0.bin\n \u2523 \ud83d\udcdc&lt;DeviceName&gt;_1.bin\n \u2523 ...\n \u2523 \ud83d\udcdc&lt;DeviceName&gt;_&lt;Reg&gt;.bin\n \u2517 \ud83d\udcdcdevice.yml\n</code></pre> <p>where:</p> <ul> <li><code>&lt;DeviceName&gt;</code> will be derived from the <code>device.yml</code> metadata file that fully defines the device and can be found in the repository of each device (e.g.). This file can be seen as the \"ground truth\" specification of the device. It is used to automatically generate documentation, interfaces and data ingestion tools.</li> <li><code>&lt;Device&gt;</code> should match the name of the device in the <code>rig.json</code> schema file;</li> <li><code>&lt;Reg&gt;</code> is the register number that is logged in the binary file.</li> </ul>"},{"location":"file_formats/harp/#on-the-deviceyml-file","title":"On the <code>device.yml</code> file","text":"<p>Including the <code>device.yml</code> file that corresponds to the device interface used to log the device's data is required. This file affords the use of diverse tooling offered by the ecosystem, in particular device-type-aware data ingestion tools. Note that while the <code>device.yml</code> file specifies the targeted hardware, firmware, and core versions, it does not guarantee that the device from which data was acquired is running those versions. This metadata should instead be queried directly from the corresponding device's registers(see protocol core registers)</p> <ul> <li>An example of an yml file can be found here: device.yml</li> <li>And the schema for the yml file can be found here: device.yml schema</li> </ul>"},{"location":"file_formats/harp/#optional-logging-of-commands","title":"Optional logging of commands","text":"<p>A critical aspect of using the Harp protocol is that for each <code>Write</code> message received by the device from the PC host, the client will echo back a Write message timestamped by the embedded device. This assumes that all messages issued by the host are received by the device and not lost in transmission. However, this is not guaranteed. In case of a lost message, the host will not receive the echo back and cannot confirm that the message was received by the device.</p> <p>To perform post-hoc quality control, we recommend also logging <code>Commands</code>. Since <code>Commands</code> are also HarpMessage types, they can be logged in the same format as data from devices. We also recommend appending a software timestamp to each <code>Command</code> message to facilitate pairing requests with responses post-hoc.</p>"},{"location":"file_formats/harp/#application-notes","title":"Application notes","text":"<p>All harp devices, regardless of their specific application, are expected to be logged according to the following standards:</p>"},{"location":"file_formats/harp/#clock-synchronization","title":"Clock Synchronization","text":"<p>We will only consider two operational modes for devices used in the AIND: <code>Standalone</code> and <code>Synchronized</code></p> <ul> <li>In <code>Standalone</code> mode the Harp device is not subordinate to a distributed clock, and all messages are timestamped by the device's internal clock. This mode can be used if only a single Harp device is used during experiment acquisition.</li> <li>In <code>Synchronized</code> mode, the Harp device is subordinate to a distributed clock, and all messages are timestamped by the distributed clock. This mode should be used when multiple Harp devices are used during experiment acquisition. In each experiment, there shall be a single clock generator source (e.g. WhiteRabbit device) to which all devices are connected. This source device is logged like any other Harp device.</li> </ul>"},{"location":"file_formats/harp/#interfacing-with-bonsai","title":"Interfacing with Bonsai","text":"<p>The following points will describe recommendations and recipes for logging data from harp devices using Bonsai programming language. We will assume a basic understanding of the Bonsai programming language, and how to interface with Harp devices from it.</p> <p>Instructions on how to log data from a Harp device using Bonsai can be found in the Harp Bonsai interface docs. A \"syntactic sugar\" operator for logging device data with a corresponding device.yml file is also available in via the AllenNeuralDynamics.HarpUtils package.</p> <p>Important</p> <p>In your experiments, always validate that your logging routine has fully initialized before requesting a reading dump from the device. Failure to do so may result in missing data.</p> <p>Note</p> <p>In the future we will update these recipes to also provide AIND specific examples.</p> <p>It is critical that the messages logged from the device are sufficient to reconstruct its state history. For that to be true, we need to know the initial state of all registers. This can be asked via a special register in the protocol core: <code>OperationControl</code>. This register has a single bit that, when set, will trigger the device to send a dump all the values of all its registers.</p> <ul> <li>To the previous logging example, in a different branch:</li> <li>Add a <code>Timer</code> operator with its <code>DueTime</code> property set to 2 seconds. This will mimic the delayed start of an experiment.</li> <li>Add a <code>CreateMessage(Bonsai.Harp)</code> operator after the <code>Timer</code></li> <li>Select <code>OperationControlPayload</code> under <code>Payload</code>. Depending on your use case, you might want to change some of the settings, but we recommend:</li> <li><code>DumpRegisters</code> set to <code>True</code> (Required for the dump)</li> <li><code>Heartbeat</code> set to <code>True</code> (Useful to know the device is still alive)</li> <li><code>MuteReplies</code> set to <code>False</code></li> <li><code>OperationLed</code> set to <code>True</code></li> <li><code>OperationMode</code> set to <code>Active</code></li> <li><code>VisualIndicator</code> set to <code>On</code></li> <li>Add a <code>Multicast</code> operator to send the message to the device</li> </ul> Workflow (Copy+Paste me) <pre><code>&lt;?xml version=\"1.0\" encoding=\"utf-8\"?&gt;\n&lt;WorkflowBuilder Version=\"2.8.1\"\n                 xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\"\n                 xmlns:rx=\"clr-namespace:Bonsai.Reactive;assembly=Bonsai.Core\"\n                 xmlns:harp=\"clr-namespace:Bonsai.Harp;assembly=Bonsai.Harp\"\n                 xmlns=\"https://bonsai-rx.org/2018/workflow\"&gt;\n  &lt;Workflow&gt;\n    &lt;Nodes&gt;\n      &lt;Expression xsi:type=\"Combinator\"&gt;\n        &lt;Combinator xsi:type=\"rx:Timer\"&gt;\n          &lt;rx:DueTime&gt;PT2S&lt;/rx:DueTime&gt;\n          &lt;rx:Period&gt;PT0S&lt;/rx:Period&gt;\n        &lt;/Combinator&gt;\n      &lt;/Expression&gt;\n      &lt;Expression xsi:type=\"harp:CreateMessage\"&gt;\n        &lt;harp:MessageType&gt;Write&lt;/harp:MessageType&gt;\n        &lt;harp:Payload xsi:type=\"harp:CreateOperationControlPayload\"&gt;\n          &lt;harp:OperationMode&gt;Active&lt;/harp:OperationMode&gt;\n          &lt;harp:DumpRegisters&gt;false&lt;/harp:DumpRegisters&gt;\n          &lt;harp:MuteReplies&gt;false&lt;/harp:MuteReplies&gt;\n          &lt;harp:VisualIndicators&gt;Off&lt;/harp:VisualIndicators&gt;\n          &lt;harp:OperationLed&gt;Off&lt;/harp:OperationLed&gt;\n          &lt;harp:Heartbeat&gt;Disabled&lt;/harp:Heartbeat&gt;\n        &lt;/harp:Payload&gt;\n      &lt;/Expression&gt;\n      &lt;Expression xsi:type=\"MulticastSubject\"&gt;\n        &lt;Name&gt;BehaviorCommands&lt;/Name&gt;\n      &lt;/Expression&gt;\n    &lt;/Nodes&gt;\n    &lt;Edges&gt;\n      &lt;Edge From=\"0\" To=\"1\" Label=\"Source1\" /&gt;\n      &lt;Edge From=\"1\" To=\"2\" Label=\"Source1\" /&gt;\n    &lt;/Edges&gt;\n  &lt;/Workflow&gt;\n&lt;/WorkflowBuilder&gt;\n</code></pre> <p>Finally, commands to the device can be logged in the exact same way as replies. However, in order to facilitate post-hoc quality control, we recommend appending a software timestamp to each <code>Command</code> message. This can be done by \"injecting\" a timestamp into the message payload before logging. We recommend using high frequency events from a single device as a source of \"the latest timestamp\" to be used in the <code>Command</code> message. We should stress that these timestamps should not be used for analysis that require precise and accurate synchronization, as they are not synchronized with the distributed clock.</p>"},{"location":"file_formats/harp/#relationship-to-aind-data-schema","title":"Relationship to aind-data-schema","text":"<p>Most fields tracked in <code>rig.json</code> can be easily extracted from the device's read-dump. It is likely that helper methods will be provided in the future to automate this conversion. For now, refer to the protocol's core registers to extract the necessary information.</p>"},{"location":"file_formats/harp/#file-quality-assurances","title":"File Quality Assurances","text":"<p>By virtue of implementing the Harp communication and synchronization protocol the following should be true:</p> <p>1) Each data set should, at most, have a device as a source of the synchronized clock. 2) All messages from the device to the computer host should be logged. Once a message is successfully parsed, no more processing and/or filtering of the data stream will be done prior to logging. 3) All data from a single device will include the initial state of all registers. This can be achieved by setting the <code>DumpRegisters</code> bit in the <code>OperationControl</code> register. Given that this is true, inside the container folder, one file per register of the device is expected to be found with a minimum of one message in each file. 4) If Commands are logged, for each message sent to the device, a corresponding message should exist in the logged data from the harp device. The type of the message in the Command will match the type of the reply from the device. 5) If multiple devices are used, all data is assumed to be synchronized at acquisition time.</p>"},{"location":"file_formats/nwb/","title":"Standards on Neurodata Without Borders files","text":""},{"location":"file_formats/nwb/#version","title":"Version","text":"<p><code>0.1.0-draft</code></p>"},{"location":"file_formats/nwb/#introduction","title":"Introduction","text":"<p>Neurodata Without Borders (NWB) files are a standard file format for neurophysiology data, however there are many topics where NWB is not opinionated. This documents defines AIND's opinions. </p>"},{"location":"file_formats/nwb/#basics","title":"Basics","text":"<p>We also take this opportunity to remind readers about some general NWB basics:</p> <ul> <li>Raw data goes in the <code>acquisition</code> group</li> <li>Processed data goes in the <code>processing</code> group</li> <li>The root level <code>session_start_time</code> is a timezone-aware datetime defining the start of acquisition.</li> <li>All timestamps must be relative to the same point in time and require no additional alignment to compare. This is defined in the root level <code>timestamps_reference_time</code>, which defaults to <code>session_start_time</code>.</li> <li>It is not required that timestamps for different data streams be identical (i.e. resample data such that all timeseries have exactly the same timestamps).</li> </ul>"},{"location":"file_formats/nwb/#harp","title":"HARP","text":"<ul> <li>At AIND, if there is a HARP board available, timestamps must utilize HARP TTLs to transform timestamps to a common timebase. It is insufficient to store a lookup table.</li> <li>If it is necessary to preserve the original, misaligned timestamps for a timeseries, they can be stored separately in an appropriately named <code>DynamicTable</code>, e.g. <code>ephys_temporal_alignment</code> with an <code>original_timestamps</code> column. </li> </ul>"},{"location":"file_formats/nwb/#events","title":"Events","text":"<ul> <li>Any events should be packaged using the ndx-events NWB extension.</li> <li>We represent all discrete events in a single a <code>EventTable</code> named <code>events</code>.</li> <li>Events in the events table have a <code>timestamp</code> property, along with arbitrary property names (columns) and values.</li> <li>All event property values and their descriptions are stored in single a <code>MeaningsTable</code> table named <code>event_descriptions</code> (e.g. property name \"lick\" with value \"0\" means \"the mouse licked the left water port\").</li> <li>All event property values must be described by HED tags or part of AIND's HED extension.</li> <li>Trials are represented as events, but also fill in the event <code>duration</code> property.</li> <li>Continuous data (e.g. running wheel velocity) are stored in <code>TimeSeries</code> arrays, not event tables.</li> </ul>"},{"location":"file_formats/nwb/#application-notes","title":"Application Notes","text":"<p>BIDS and HED have defined two file formats that can generically describe events in a task-agnostic fashion. </p> <p>First, events.csv, which describes individual events:  <pre><code>| onset  | duration | HED | &lt;additional columns&gt; | ... |\n--------------------------------------------------\n| float  | float    | str | ...                  | ... |\n</code></pre></p> <p>Second, event-descriptions.csv, which describes event types and their values: <pre><code>| column_name | column_value | description | HED |\n--------------------------------------------------\n| str         | Any          | str         | str |\n</code></pre></p> <p>BIDS likewise has a standard JSON representations of event descriptions.</p> <p>A general-purpose, task-agnostic utility for could write this in to NWB without any custom code necessary (for example).</p>"},{"location":"file_formats/nwb/#relationship-to-aind-data-schema","title":"Relationship to aind-data-schema","text":"<p>The AIND HED extension will likely live in the aind-data-schema-models repository.</p>"},{"location":"file_formats/nwb/#file-quality-assurances","title":"File Quality Assurances","text":"<p>None.</p>"},{"location":"file_formats/planar_ophys/","title":"Standards on planar optical physiology acquisition","text":"<p>Version: 0.0.1</p>"},{"location":"file_formats/planar_ophys/#acquisitionraw-data-format","title":"Acquisition/Raw Data Format","text":"<p>Planar optical physiology data should be saved to a directory called pophys. Files should be in raw TIFF format (v4) with optional, related images (surface, depth, local and cortical stacks) and application metadata in the same directory. </p> <p>Single Plane</p> <p>Trials are 2-photon movies captured as a series of TIFF files with a maximum size of 2000 frames per TIFF.</p> <p>Three experimental epochs are collected during a session. Each can contain a variety of trials. Experimental epochs are called: Spontaneous, Single Neuron BCI Conditioning and 2p Photostimulation.</p> <p>A local TIFF stack is acquired above and below (40um) the ROI for motion correction and for locating the FOV on successive days.</p> <p>.mat files saved for each Single Neuron BCI Conditioning trial and contains threshold parameters for BCI mapping.</p> <p>Scan Image outputs a file detecting the frame shift of the FOV to motion correct ROI (.csv).</p> <p>Scan Image saves the power of laser used in 2p photostimulation (.stim file); analog voltage signal sampled at 9kHz.</p> <p>Full activity traces for the duration of the BCI experiment of ROI of the conditioned neurons and ~30 other neurons considered as candidates for the conditioned neuron (.csv).</p> <pre><code>\ud83d\udce6pophys\n \u2523 \ud83d\udcdcspont_000n.tiff\n \u2523 \ud83d\udcdcneuron_000n.tiff\n \u2523 \ud83d\udcdcphotostim_000n.tiff\n \u2523 \ud83d\udcdclocal.tiff\n \u2523 \ud83d\udcdcneuron.mat\n \u2523 \ud83d\udcdccorrection.csv\n \u2523 \ud83d\udcdcactivity.csv\n \u2523 \ud83d\udcdcphotostim_power.stim\n</code></pre> <p>Multiplane</p> <p>Epi-fluorescence surface vasuculature image is acquired for session-to-session matching and saved in TIFF format.</p> <p>2-photon surface image stored as a TIFF for each ROI where the image contains interleaved images of the 2p ROI surface acquired in one channel. Each ROI will be on it's TIFF page. 16 frames per ROI (See Figure 1).</p> <p>2-photon depth image are stored TIFF for each plane. Images are acquired in 2 channels and each plane will be on it's own page in the TIFF stack where the orders are P1, C1, P1, C2 (See Figure 1). There are 16 frames per plane (2x longer than the surface image)</p> <p>There is one movie file for a stimulus presented during a session. The timeseries image is acquired as a TIFF in the same order as the depth image (See Figure 1). The frame rate of acquisition will vary based on how many paired planes are being imaged.</p> <p>Local z-stacks are in TIFF formats and are acquired by pairs. The number of z-stacks equals the number of paired planes. The contents are interleaved and acquired in both channels (P1, C1, P1, C2).</p> <p>Cortical stack for each ROI and they are acquired in one channel. </p> <p>A fullfield TIFF file is used for registration of the ROI, surface 2p files for visual reistration. The fullfield scans the entire FOV. </p> <p>ROI files are stored with all images acquired by Scan Image and contain cordinates for each ROI and scanfield for each non-stack TIFF files (surface, depth, timeseries and fullfield). The ROI files are loaded by users for day-to-day targeting to help operators know the location of each ROI.</p> <pre><code>\ud83d\udce6pophys\n \u2523 \ud83d\udcdcmouse-id_YYYY-MM-DD_HH-MM-SS_averaged_depth.tiff\n \u2523 \ud83d\udcdcmouse-id_YYYY-MM-DD_HH-MM-SS_averaged_surface.tiff\n \u2523 \ud83d\udcdcmouse-id_YYYY-MM-DD_HH-MM-SS_fullfield.roi\n \u2523 \ud83d\udcdcmouse-id_YYYY-MM-DD_HH-MM-SS_fullfield.tiff\n \u2523 \ud83d\udcdcmouse-id_YYYY-MM-DD_HH-MM-SS_local_z_stack0.tiff\n \u2523 \ud83d\udcdcmouse-id_YYYY-MM-DD_HH-MM-SS_surface.roi\n \u2523 \ud83d\udcdcmouse-id_YYYY-MM-DD_HH-MM-SS_timeseries.roi\n \u2523 \ud83d\udcdcmouse-id_YYYY-MM-DD_HH-MM-SS_timeseries.tiff\n \u2517 \ud83d\udcdcmouse-id_YYYY-MM-DD_HH-MM-SS_vasculature.tif\n</code></pre> <p> Figure 1</p>"},{"location":"file_formats/planar_ophys/#application-notes","title":"Application notes","text":"<p>ScanImage major version &gt;= 2020</p>"},{"location":"file_formats/planar_ophys/#relationship-to-aind-data-schema-session","title":"Relationship to aind-data-schema Session","text":"<p>Power configurations and frame rate(s) are stored in the session.json under the ophys_fovs key within data_streams. Laser configuration and frame rates are recorded for each FOV. In the case of multiplane acquisition, associated pairs are linked under the coupled_fov_index and ROI FOV is recorded under scanimage_roi_index. The index field is used to count how many FOVs were acquired in a single session.</p>"},{"location":"file_formats/planar_ophys/#primary-data-format","title":"Primary Data Format","text":"<p>Raw movie files are converted to HDF5 files. HDF5 files contain movie from one plane only. Within the HDF5 file, the movie is stored as the data dataset and metadata from the TIFF header are stored as the metadata dataset.</p> <p>The conversion from TIFF to HDF5 happens before upload to cloud. Multiplane data must be de-interleaved into separate planes and saved as HDF5 while single-plane images must be stitched together from the array of TIFFs collected on the rig.</p> <p>Single plane HDF5s with multiple stimulus epochs are annotated using the datasets tiff_stem_location and epoch_location. tiff_stem_location contains the frames associated with each TIFF stem name in the data dataset while the epoch_location records the frames in the dataset associated with each epoch (spontaneous activity, single neuron BCI activity and 2p photostimulation). Single plane HDF5s also contain one TIFF header for each unique, TIFF stem type.</p> <p>To specify a reference image for processing a \"reference_image.h5\" file must be specified as a virtual, HDF5 dataset in the same directory as the primary data format.</p> <p>Single Plane File Structure</p> <pre><code>\ud83d\udce6pophys\n \u2523 \ud83d\udcdcmouse-id_YYYY-MM-DD_HH-MM-SS.h5\n \u2523 \ud83d\udcdcmouse-id_YYYY-MM-DD_HH-MM-SS_VDS.h5\n \u2523 \ud83d\udcdcmouse-id_YYYY-MM-DD_HH-MM-SS_reference_image.h5\n \u2523 \ud83d\udcdcmouse-id_YYYY-MM-DD_HH-MM-SS_local-stack.h5\n \u2523 \ud83d\udcdcmouse-id_YYYY-MM-DD_HH-MM-SS_neuron.mat\n \u2523 \ud83d\udcdcmouse-id_YYYY-MM-DD_HH-MM-SS_correction.csv\n \u2523 \ud83d\udcdcmouse-id_YYYY-MM-DD_HH-MM-SS_activity.csv\n \u2523 \ud83d\udcdcmouse-id_YYYY-MM-DD_HH-MM-SS_photostim_power.stim\n</code></pre> <p>Multiplane File Structure</p> <pre><code>\ud83d\udce6pophys\n \u2523 \ud83d\udcc2VISP0_1\n \u2503 \u2523 \ud83d\udcdcmouse-id_YYYY-MM-DD_HH-MM-SS.h5\n \u2503 \u2523 \ud83d\udcdcmouse-id_YYYY-MM-DD_HH-MM-SS_averaged_depth.h5\n \u2503 \u2523 \ud83d\udcdcmouse-id_YYYY-MM-DD_HH-MM-SS_averaged_surface.h5\n \u2503 \u2517 \ud83d\udcdcmouse-id_YYYY-MM-DD_HH-MM-SS_local-stack.h5\n \u2523 \ud83d\udcc2VISP0_2\n \u2503 \u2523 \ud83d\udcdcmouse-id_YYYY-MM-DD_HH-MM-SS.h5\n \u2503 \u2523 \ud83d\udcdcmouse-id_YYYY-MM-DD_HH-MM-SS_averaged_depth.h5\n \u2503 \u2523 \ud83d\udcdcmouse-id_YYYY-MM-DD_HH-MM-SS_averaged_surface.h5\n \u2503 \u2517 \ud83d\udcdcmouse-id_YYYY-MM-DD_HH-MM-SS_local-stack.h5\n \u2523 \ud83d\udcc2VISP0_3\n \u2503 \u2523 \ud83d\udcdcmouse-id_YYYY-MM-DD_HH-MM-SS.h5\n \u2503 \u2523 \ud83d\udcdcmouse-id_YYYY-MM-DD_HH-MM-SS_averaged_depth.h5\n \u2503 \u2523 \ud83d\udcdcmouse-id_YYYY-MM-DD_HH-MM-SS_averaged_surface.h5\n \u2503 \u2517 \ud83d\udcdcmouse-id_YYYY-MM-DD_HH-MM-SS_local-stack.h5\n \u2523 \ud83d\udcc2VISP0_4\n \u2503 \u2523 \ud83d\udcdcmouse-id_YYYY-MM-DD_HH-MM-SS.h5\n \u2503 \u2523 \ud83d\udcdcmouse-id_YYYY-MM-DD_HH-MM-SS_averaged_depth.h5\n \u2503 \u2523 \ud83d\udcdcmouse-id_YYYY-MM-DD_HH-MM-SS_averaged_surface.h5\n \u2503 \u2517 \ud83d\udcdcmouse-id_YYYY-MM-DD_HH-MM-SS_local-stack.h5\n \u2517 \ud83d\udcdcmouse-id_YYYY-MM-DD_HH-MM-SS_cortical-stack1.h5\n</code></pre>"},{"location":"file_formats/planar_ophys/#application-notes_1","title":"Application notes","text":"<p>aind-ophys-mesoscope-image-splitter</p> <p>aind-ophys-bergamo-stitcher</p>"},{"location":"file_formats/planar_ophys/#file-quality-assurance","title":"File Quality Assurance","text":"<p>The number of frames in each HDF5 file should match the number of frames in the TIFF files.</p> <p>The acquisition start time and end time should be consistent with the frames acquired at a given frame rate.</p>"},{"location":"file_formats/planar_ophys/#vocabulary","title":"Vocabulary","text":"<p>Single Plane</p> <p>FOV: Field of view of the imaging region. ROI: Region of interest pertaining to conditioned cell.</p> <p>Multiplane</p> <p>FOV: Field of view of the entire brain. ROI: Region of interest pertaining to the plane being imaged.</p>"}]}